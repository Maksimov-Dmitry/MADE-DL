{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://raw.githubusercontent.com/lysukhin/MADE/2022/computer_vision/seminar06_segmentation/download.sh\n",
    "!bash download.sh\n",
    "!pip install pytorch_lightning segmentation_models_pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загузите файл checkpoints.zip\n",
    "from google.colab import files\n",
    "uploaded = files.upload()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img style=\"float: left;\" src=\"resources/made.jpg\" width=\"25%\" height=\"25%\">\n",
    "\n",
    "# Академия MADE\n",
    "## Семинар 6: сегментация\n",
    "Иван Карпухин, ведущий исследователь-программист\n",
    "\n",
    "<div style=\"clear:both;\"></div>\n",
    "\n",
    "В рамках семинар обучим U-Net модель отделения предметов от фона на корпусе Pascal VOC.\n",
    "\n",
    "Для выполнения работы нужны следующие пакеты (Python 3):\n",
    "* opencv-python\n",
    "* torch\n",
    "* torchvision\n",
    "* segmentation-models-pytorch\n",
    "\n",
    "Установить их можно командой:\n",
    "```bash\n",
    "pip3 install --user opencv-python torch torchvision segmentation-models-pytorch\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://data.deepai.org/PascalVOC2012.zip\n",
    "!unzip PascalVOC2012.zip\n",
    "!mkdir VOCdevkit\n",
    "!mv voc2012 VOCdevkit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from zipfile import ZipFile\n",
    "if not os.path.exists(\"checkpoints.zip\"):\n",
    "    raise FileNotFoundError(\"Скачайте архив с обученными моделями: https://cloud.mail.ru/public/4XBu/vbfcjqzA6\")\n",
    "\n",
    "with ZipFile(\"checkpoints.zip\", \"r\") as zf:\n",
    "    zf.extractall(\".\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import random\n",
    "\n",
    "import cv2\n",
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import pytorch_lightning as pl\n",
    "import segmentation_models_pytorch as smp\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image\n",
    "from torchvision.datasets import VOCSegmentation\n",
    "\n",
    "import cvmade\n",
    "import seminar\n",
    "\n",
    "# Реализация аугментаций с соответствующего семинара.\n",
    "from augmentations import AffineAugmenter, BrightnessContrastAugmenter, BlurAugmenter, FlipAugmenter, RandomAugmentation\n",
    "\n",
    "print(\"Torch\", torch.__version__)\n",
    "\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "if USE_CUDA:\n",
    "    DEVICE=\"cuda\"\n",
    "    print(\"Use CUDA\")\n",
    "else:\n",
    "    DEVICE=\"cpu\"\n",
    "    \n",
    "# Параметры тренировки и визуализации.\n",
    "TRAIN = False  # Если False, будут использоваться заранее посчитанные графики\n",
    "              # вместо реальной тренировки (полезно для демонстрации на ноутбуках без GPU).\n",
    "\n",
    "def set_figure_size(figsize=(8, 6), dpi=120):\n",
    "    plt.figure(figsize=figsize, dpi=dpi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Тренировка Unet на Pascal VOC.\n",
    "\n",
    "## Подготовка данных\n",
    "\n",
    "Мы будем использовать корпус Pascal VOC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_ROOT = \".\"\n",
    "VOC_YEAR = \"2012\"\n",
    "LABELS = [\"background\", \"aeroplane\", \"bicycle\", \"bird\", \"boat\", \"bottle\",\n",
    "          \"bus\", \"car\", \"cat\", \"chair\", \"cow\", \"diningtable\", \"dog\", \"horse\",\n",
    "          \"motorbike\", \"person\", \"potted_plant\", \"sheep\", \"sofa\", \"train\", \"tv/monitor\"]\n",
    "MULTICLASS = False  # Если False, отделяем объекты от фона, иначе распознаем класс объектов.\n",
    "NUM_CLASSES = len(LABELS) if MULTICLASS else 2\n",
    "print(\"Number of classes:\", NUM_CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Посмотрим на валидационную часть корпуса.\n",
    "valset_raw = VOCSegmentation(DATA_ROOT, VOC_YEAR, \"val\", download=False)\n",
    "\n",
    "image, mask = valset_raw[random.randint(0, len(valset_raw) - 1)]\n",
    "print(\"Mask values:\", set(np.array(mask).flatten().tolist()))\n",
    "\n",
    "fig, axs = plt.subplots(1, 2)\n",
    "axs[0].imshow(image)\n",
    "axs[1].imshow(mask)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Маска содержит номера классов для каждого пикселя. Имеется две специальные метки: 0 для фона и 255 для контура. Контур мы уберем."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(5):\n",
    "    i = random.randint(0, len(valset_raw) - 1)\n",
    "    print(\"Image {:05d} size: {}\".format(i, valset_raw[i][0].size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В корпусе содержатся изображения разного размера. Можно пойти разными путями:\n",
    "1. привести изображения к одному размеру масштабированием без сохранения пропорций,\n",
    "2. привести изображения к одному размеру масштабированием с сохранением пропорций,\n",
    "3. привести изображения к одному размеру, вырезая случайные части,\n",
    "4. использовать модели, которые могут работать с разными размерами.\n",
    "\n",
    "Заметим:\n",
    "* Сеть U-Net способна обрабатывать изображения разных размеров, однако из изображений разного размера нельзя составить однородный тензор батча. Значит, изображения разного размера затруднят тренировку.\n",
    "\n",
    "* Сеть U-Net является полностью сверточной сетью без паддинга, значит каждый нейрон видит лишь часть картинки и не зависит от положения на изображении и близости к границе.\n",
    "\n",
    "* Область внимания каждого нейрона задается в пикселях. Значит, чем меньше изображение, тем больше информации о сцене получает каждый нейрон.\n",
    "\n",
    "Мы воспользуемся вторым вариантом.\n",
    "\n",
    "При масштабировании изображений с сохранением пропорций и встравивании их в квадратные изображения, образуются пустые области. Пустые области будем заполнять зеркальными отражениями."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Scaler(object):\n",
    "    \"\"\"Отмасштабировать изображения сохранив пропорции.\n",
    "    \n",
    "    Пустые места будут заполнены отражениями.\n",
    "\n",
    "    Аргументы:\n",
    "        image: Изображение в HWC формате.\n",
    "        size: Требуемый размер, пара W, H.\n",
    "    \"\"\"\n",
    "    def __init__(self, size):\n",
    "        self._size = size\n",
    "        self._padding = 0\n",
    "    \n",
    "    def set_padding(self, padding):\n",
    "        self._padding = padding\n",
    "        \n",
    "    def __call__(self, image):\n",
    "        if not isinstance(image, np.ndarray):\n",
    "            image = np.array(image)\n",
    "        grayscale = (len(image.shape) == 2)\n",
    "        if grayscale:\n",
    "            image = image[..., None]\n",
    "        rw, rh = self._size\n",
    "        p = self._padding\n",
    "        h, w, c = image.shape\n",
    "        scale_x = rw / w\n",
    "        scale_y = rh / h\n",
    "        scale = min(scale_x, scale_y)\n",
    "        sw = int(scale * w)\n",
    "        sh = int(scale * h)\n",
    "        offset_x = p + (rw - sw) // 2\n",
    "        offset_y = p + (rh - sh) // 2\n",
    "        # Используем zeros на случай маленьких изображений.\n",
    "        # TODO: фикс индексирования для маленьких изображений.\n",
    "        result = np.zeros((rh + 2 * p, rw + 2 * p, c), dtype=image.dtype)\n",
    "        cv2.resize(image, (sw, sh),\n",
    "                   interpolation=cv2.INTER_NEAREST if grayscale else cv2.INTER_AREA,\n",
    "                   dst=result[offset_y:offset_y + sh, offset_x:offset_x + sw])\n",
    "\n",
    "        # Отразить.\n",
    "        result[offset_y:offset_y + sh, :offset_x] = result[offset_y:offset_y + sh, offset_x:2 * offset_x][:, ::-1]\n",
    "        offset_end = result.shape[1] - offset_x - sw\n",
    "        result[offset_y:offset_y + sh, offset_x + sw:] = result[offset_y:offset_y + sh, sw + offset_x - offset_end:sw + offset_x][:, ::-1]\n",
    "        \n",
    "        result[:offset_y] = result[offset_y:2 * offset_y][::-1]\n",
    "        offset_end = result.shape[0] - offset_y - sh\n",
    "        result[offset_y + sh:] = result[sh + offset_y - offset_end:sh + offset_y][::-1]\n",
    "        \n",
    "        if grayscale:\n",
    "            result = result[:, :, 0]\n",
    "        return result\n",
    "\n",
    "    \n",
    "def remove_borders_inplace(mask):\n",
    "    mask[mask == 255] = 0\n",
    "    return mask\n",
    "    \n",
    "\n",
    "class VOCDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"Обертка для стандартного класса, которая форматирует данные.\"\"\"\n",
    "    def __init__(self, root, year, image_size,\n",
    "                 multiclass=MULTICLASS,\n",
    "                 download=False, augment=False,\n",
    "                 train=False, train_fraction=0.8,\n",
    "                 padding=0):\n",
    "        \"\"\"Создать корпус.\n",
    "        \n",
    "        Аргументы:\n",
    "            root: Путь до папки для хранения данных.\n",
    "            year: 2007 или 2012.\n",
    "            image_size: Ширина и высота изображений.\n",
    "            download: Скачать корпус, если он не найден в папке.\n",
    "            augment: Применить аугментации.\n",
    "            train: Выделить train часть, иначе validation.\n",
    "            train_fraction: Доля тренировочных примеров.\n",
    "            padding: Добавить дополнительный зеркальынй паддинг к изображению.\n",
    "        \"\"\"\n",
    "        self._dataset = VOCSegmentation(root, str(year), \"trainval\", download=download)\n",
    "        self._image_size = np.array(image_size)\n",
    "        self._multiclass = multiclass\n",
    "        self._padding = padding\n",
    "        \n",
    "        ids = list(range(len(self._dataset)))\n",
    "        random.seed(0)\n",
    "        random.shuffle(ids)\n",
    "        train_size = int(len(ids) * train_fraction)\n",
    "        self._indices = ids[:train_size] if train else ids[train_size:]\n",
    "\n",
    "        self._image_scaler = Scaler(image_size)\n",
    "        self._mask_scaler = Scaler(image_size)\n",
    "        if augment:\n",
    "            self._augmenter = RandomAugmentation(AffineAugmenter(),\n",
    "                                                 BrightnessContrastAugmenter(),\n",
    "                                                 BlurAugmenter(),\n",
    "                                                 FlipAugmenter())\n",
    "        else:\n",
    "            self._augmenter = lambda image, mask: (image, mask)\n",
    "        \n",
    "    @property\n",
    "    def image_size(self):\n",
    "        return self._image_size + self._padding * 2\n",
    "    \n",
    "    def set_padding(self, padding):\n",
    "        self._padding = padding\n",
    "        self._image_scaler.set_padding(padding)\n",
    "        self._mask_scaler.set_padding(padding)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self._indices)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image, mask = self._dataset[self._indices[idx]]\n",
    "        image = np.asarray(image)\n",
    "        mask = np.array(mask)\n",
    "        remove_borders_inplace(mask)\n",
    "        if not self._multiclass:\n",
    "            mask[mask > 0] = 1\n",
    "        image = self._image_scaler(image)\n",
    "        mask = self._mask_scaler(mask)\n",
    "        image, mask = self._augmenter(image, mask)\n",
    "        image = cvmade.image.image_to_torch(image).float() - 0.5\n",
    "        mask = cvmade.image.image_to_torch(mask[..., None]).long()[0]\n",
    "        return image, mask\n",
    "\n",
    "IMAGE_SIZE = 512  # В исходной статье U-Net применяется к изображениям размера 572.\n",
    "\n",
    "# Корпус был загружен выше, download = False.\n",
    "trainset = VOCDataset(DATA_ROOT, VOC_YEAR, (IMAGE_SIZE, IMAGE_SIZE),\n",
    "                      multiclass=MULTICLASS, train=True)\n",
    "valset = VOCDataset(DATA_ROOT, VOC_YEAR, (IMAGE_SIZE, IMAGE_SIZE),\n",
    "                    multiclass=MULTICLASS, train=False)\n",
    "print(\"Image size:\", IMAGE_SIZE)\n",
    "print(\"Train set size:\", len(trainset))\n",
    "print(\"Validation set size:\", len(valset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Корпус очень маленький.\n",
    "1. Мы будем учить сеть, состоящую только из сверток. У таких сетей относительно мало параметров и они могут обучаться на небольших корпусах.\n",
    "2. Каждый пиксель - отдельная метка. Количество пикселей в корпусе огромно, в этом смысле корпус большой.\n",
    "3. Добавим аугментации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def show_dataset(dataset):\n",
    "    images = []\n",
    "    masks = []\n",
    "    for _ in range(5):\n",
    "        i = random.randint(0, len(valset) - 1)\n",
    "        if len(images):\n",
    "            separator = np.zeros((dataset.image_size[0], 2, 3), dtype=np.uint8)\n",
    "            images.append(separator)\n",
    "            masks.append(separator[:, :, 0])\n",
    "        image, mask = dataset[i]\n",
    "        images.append(cvmade.image.image_to_numpy(image))\n",
    "        masks.append(cvmade.image.image_to_numpy(mask[None, ...])[..., 0])\n",
    "\n",
    "    %matplotlib inline\n",
    "    set_figure_size()\n",
    "    plt.imshow(np.concatenate(images, axis=1))\n",
    "    plt.show()\n",
    "    set_figure_size()\n",
    "    plt.imshow(np.concatenate(masks, axis=1))\n",
    "    plt.show()\n",
    "    \n",
    "print(\"No padding\")\n",
    "valset.set_padding(0)\n",
    "show_dataset(trainset)\n",
    "print(\"Paddng\")\n",
    "valset.set_padding(40)\n",
    "show_dataset(trainset)\n",
    "valset.set_padding(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Создание модели\n",
    "\n",
    "Создадим сеть вида U-Net ( https://arxiv.org/pdf/1505.04597.pdf ).\n",
    "\n",
    "Сеть U-Net состоит из двух частей. Первая часть строит эмбеддинг. Вторая часть зарисовывает изображение и имеет структуру, похожую на первую часть. Первая часть использует max pooling и сжимает изображение, а вторая разжимает за счет транспонированной свертки. Из-за этого возникает параллель между слоями первой и второй части: они имеют схожий размер тензоров. По этой причине мы можем использовать выходы слоев первой части в качестве входов для слоев второй части. Информация из первой части, в которой сохраняются небольшие детали изображения, без труда используется при зарисовке, что приводит к повыжению детализации маски.\n",
    "\n",
    "<img src=\"resources/example-image.jpg\" align=\"left\" hspace=\"20\" width=\"20%\" height=\"20%\"/> \n",
    "<img src=\"resources/u-net.jpg\" align=\"left\" hspace=\"20\" width=\"50%\" height=\"50%\"/> \n",
    "<img src=\"resources/example-mask.jpg\" align=\"left\" hspace=\"20\" width=\"20%\" height=\"20%\"/> \n",
    "<div style=\"clear:both;\"></div>\n",
    "\n",
    "Почему сеть называется U-Net?\n",
    "\n",
    "Заметьте, что размер выходного тензора меньше, чем размер исходного. В сети не используется паддинг, что позволяет сделать выход независящим от позиции на изображении. В оригинальной статье сеть используется для сегментации больших изображений методом скользящего окна. При этом на границе используется зеркальный padding, размер которого определяется receptive field сети.\n",
    "\n",
    "<img src=\"resources/unet-window.jpg\" align=\"left\" hspace=\"20\" width=\"80%\" height=\"80%\"/> \n",
    "<div style=\"clear:both;\"></div>\n",
    "\n",
    "Мы внесем несколько изменений в U-Net для ускорения тренировки и повышения качества:\n",
    "* будем использовать меньший размер изображений и меньший размер сети.\n",
    "* Добавим residual connections.\n",
    "* Добавим batch normalization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Шпаргалка:\n",
    "\n",
    "```python\n",
    "torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros')\n",
    "\n",
    "torch.nn.ReLU(inplace=False)\n",
    "\n",
    "torch.nn.MaxPool2d(kernel_size, stride=None, padding=0, dilation=1, return_indices=False, ceil_mode=False)\n",
    "\n",
    "torch.nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride=1, padding=0, output_padding=0, groups=1, bias=True, dilation=1, padding_mode='zeros')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_conv1x1(in_channels, out_channels):\n",
    "    \"\"\"Создать слой свертки 1x1.\"\"\"\n",
    "    # Начало вашего кода.\n",
    "    \n",
    "    layer = ...\n",
    "    \n",
    "    # Конец вашего кода.\n",
    "    \n",
    "    return layer\n",
    "\n",
    "def make_transposed_convolution2x2(in_channels, out_channels):\n",
    "    \"\"\"Создать транспонированную свертку (AKA deconvolution, upconvolution).\"\"\"\n",
    "    \n",
    "    # Обратите внимание на параметр output_padding. Поскольку stride в maxpooling\n",
    "    # может давать один и тот же размер выхода для разных размеров входа, необходимо\n",
    "    # указать такую добавку к размерности, чтобы получился тензор той же размерности,\n",
    "    # что и в первой части сети.\n",
    "    #\n",
    "    # Поскольку у нас размер изображения всегда четный,\n",
    "    # output_padding нужно выставить в 0.\n",
    "    \n",
    "    # Начало вашего кода.\n",
    "    \n",
    "    layer = ...\n",
    "    \n",
    "    # Конец вашего кода.\n",
    "    \n",
    "    return layer\n",
    "\n",
    "def make_conv3x3(in_channels, out_channels, bias=True):\n",
    "    return torch.nn.Conv2d(in_channels, out_channels, 3, bias=bias)\n",
    "\n",
    "def make_batchnorm(channels):\n",
    "    return torch.nn.BatchNorm2d(channels)\n",
    "\n",
    "def make_relu():\n",
    "    return torch.nn.ReLU(inplace=True)\n",
    "\n",
    "def make_maxpool2x2():\n",
    "    return torch.nn.MaxPool2d(2)\n",
    "\n",
    "seminar.check_conv1x1(make_conv1x1)\n",
    "seminar.check_t_conv(make_transposed_convolution2x2)\n",
    "seminar.check_ok()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock(torch.nn.Sequential):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__(\n",
    "            make_conv3x3(in_channels, out_channels, bias=False),\n",
    "            make_batchnorm(out_channels),\n",
    "            make_relu(),\n",
    "            make_conv3x3(out_channels, out_channels, bias=False),\n",
    "            make_batchnorm(out_channels),\n",
    "            make_relu()\n",
    "        )\n",
    "        \n",
    "class ResConvBlock(torch.nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.main_path = torch.nn.Sequential(\n",
    "            make_conv3x3(in_channels, out_channels, bias=False),\n",
    "            make_batchnorm(out_channels),\n",
    "            make_relu(),\n",
    "            make_conv3x3(out_channels, out_channels, bias=False),\n",
    "            make_batchnorm(out_channels)\n",
    "        )\n",
    "        self.residual_path = make_conv1x1(in_channels, out_channels)\n",
    "        self.last_relu = make_relu()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        result = self.main_path(x)\n",
    "        residual = self.residual_path(x)[:, :, 2:-2, 2:-2]\n",
    "        result = self.last_relu(result + residual)\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Мы реализуем два класса. Down реализует преобразование между двумя max pooling,\n",
    "# при этом начинается с max pooling. Первые две свертки сети в него не входят.\n",
    "# Up реализует преобразование между двумя транспонированными свертками и начинает с\n",
    "# транспонированной свертки. Последняя свертка сети в него не входит.\n",
    "#\n",
    "# Схема модели:\n",
    "# input -> conv -> conv -> Down x 4 -> Up x 4 -> conv -> output.\n",
    "\n",
    "class UNetDown(torch.nn.Sequential):\n",
    "    \"\"\"Часть сети между двумя max pooling, которая начинается с max pooling.\"\"\"\n",
    "    def __init__(self, in_channels, out_channels, block):\n",
    "        layers = [\n",
    "            make_maxpool2x2(),\n",
    "            block(in_channels, out_channels)\n",
    "        ]\n",
    "        super().__init__(*layers)\n",
    "     \n",
    "    \n",
    "class UNetUp(torch.nn.Module):\n",
    "    \"\"\"Часть сети между двумя транспонированными свертками, которая начинается с\n",
    "    транспонированной свертки.\"\"\"\n",
    "    def __init__(self, in_channels, out_channels, block):\n",
    "        super().__init__()\n",
    "        self._transposed_convolution = make_transposed_convolution2x2(in_channels, out_channels)\n",
    "        self._convolutions = block(out_channels * 2, out_channels)\n",
    "    \n",
    "    def forward(self, x_down, x_up):\n",
    "        x_up = self._transposed_convolution(x_up)\n",
    "        \n",
    "        b_down, c_down, h_down, w_down = x_down.shape\n",
    "        b_up, c_up, h_up, w_up = x_up.shape\n",
    "        if (h_up > h_down) or (w_up > w_down):\n",
    "            raise ValueError(\"Up tensor must be smaller than down tensor\")\n",
    "        offset = ((h_down - h_up) // 2, (w_down - w_up) // 2)\n",
    "        x_down_cropped = x_down[:, :, offset[0]:offset[0] + h_up, offset[1]:offset[1] + w_up]\n",
    "        \n",
    "        x = torch.cat((x_down_cropped, x_up), axis=1)\n",
    "        result = self._convolutions(x)\n",
    "        return result\n",
    "\n",
    "\n",
    "class UNet(torch.nn.Module):\n",
    "    def __init__(self, num_classes, num_scales=4, base_filters=64, block=ConvBlock):\n",
    "        \"\"\"Создать U-Net сеть.\n",
    "        \n",
    "        Параметры:\n",
    "            num_classes: Число классов на выходе. Для классификации объект/фон нужно два класса.\n",
    "            num_scales: Число блоков U-Net сети, выполняющих изменение размера изображения.\n",
    "            base_filters: Число фильтров на первом уровне сети.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self._input_convolutions = block(3, base_filters)\n",
    "        \n",
    "        layers = []\n",
    "        filters = base_filters\n",
    "        for i in range(num_scales):\n",
    "            layers.append(UNetDown(filters, filters * 2, block))\n",
    "            filters *= 2\n",
    "        self._down_layers = torch.nn.Sequential(*layers)\n",
    "        \n",
    "        layers = []\n",
    "        for i in range(num_scales):\n",
    "            layers.append(UNetUp(filters, filters // 2, block))\n",
    "            filters //= 2\n",
    "        self._up_layers = torch.nn.Sequential(*layers)\n",
    "        \n",
    "        self._output_convolution = make_conv1x1(filters, num_classes)\n",
    "        self.initialize_weights()\n",
    "\n",
    "        # Оценим насколько сеть уменьшает изображение.\n",
    "        self.eval()\n",
    "        sample_input = torch.zeros((1, 3, 1000, 1000))\n",
    "        if USE_CUDA:\n",
    "            sample_input = sample_input.cuda()\n",
    "            self.cuda()\n",
    "        with torch.no_grad():\n",
    "            sample_output = self(sample_input)\n",
    "        self.padding = (sample_input.shape[-1] - sample_output.shape[-1]) // 2\n",
    "        \n",
    "    def forward(self, x):\n",
    "        down_results = [self._input_convolutions(x)]\n",
    "        for layer in self._down_layers:\n",
    "            down_results.append(layer(down_results[-1]))\n",
    "        x = down_results[-1]\n",
    "        for i, layer in enumerate(self._up_layers):\n",
    "            x = layer(down_results[-2 - i], x)\n",
    "        x = self._output_convolution(x)\n",
    "        return x\n",
    "     \n",
    "    def initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, (torch.nn.Conv2d, torch.nn.ConvTranspose2d)):\n",
    "                torch.nn.init.kaiming_normal_(m.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n",
    "                if m.bias is not None:\n",
    "                    torch.nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, torch.nn.BatchNorm2d):\n",
    "                torch.nn.init.constant_(m.weight, 1)\n",
    "                torch.nn.init.constant_(m.bias, 0)\n",
    "        \n",
    "def count_parameters(model):\n",
    "    total = 0\n",
    "    for p in model.parameters():\n",
    "        total += np.prod(list(p.shape))\n",
    "    return total\n",
    "\n",
    "USE_RESIDUALS = True\n",
    "\n",
    "unet = UNet(NUM_CLASSES if NUM_CLASSES > 2 else 1,\n",
    "            num_scales=4,  # Число блоков U-Net сети, в статье 4.\n",
    "            base_filters=64,  # Размер свертки на первом уровне, в статье 64.\n",
    "            block=ResConvBlock if USE_RESIDUALS else ConvBlock)\n",
    "\n",
    "print(unet)\n",
    "print(\"Required padding: {}\".format(unet.padding))\n",
    "print(\"Total parameters: {}\".format(count_parameters(unet)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обучение\n",
    "\n",
    "Создадим класс для подсчета функции потерь - бинарной перекрестной энтропии."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_images(predicted, masks_or_images):\n",
    "    \"\"\"Если выход больше или меньше чем исходное изображение,\n",
    "    вырезать центральную часть из обоих, чтобы размеры совпадали.\n",
    "    \"\"\"\n",
    "    if len(masks_or_images.shape) == 3:\n",
    "        predicted, masks = crop_images(predicted, masks_or_images.unsqueeze(1))\n",
    "        return predicted, masks[:, 0]\n",
    "    images = masks_or_images\n",
    "    if (len(predicted.shape) != 4) or (len(images.shape) != 4):\n",
    "        raise ValueError(\"Expected tensors of shape BCHW\")\n",
    "    bi, ci, hi, wi = images.shape\n",
    "    bp, cp, hp, wp = predicted.shape\n",
    "    offset = (abs(hi - hp) // 2, abs(wi - wp) // 2)\n",
    "    if hp < hi:\n",
    "        images = images[:, :, offset[0]:offset[0] + hp]\n",
    "    else:\n",
    "        predicted = predicted[:, :, offset[0]:offset[0] + hi]\n",
    "    if wp < wi:\n",
    "        images = images[:, :, :, offset[1]:offset[1] + wp]\n",
    "    else:\n",
    "        predicted = predicted[:, :, :, offset[1]:offset[1] + wi]\n",
    "    return predicted, images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Шпаргалка:\n",
    "\n",
    "```python\n",
    "torch.nn.BCEWithLogitsLoss(weight=None, size_average=None, reduce=None, reduction='mean', pos_weight=None)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BCELoss(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Ваш код здесь.\n",
    "        \n",
    "        self._bce = ...\n",
    "        \n",
    "        # Конец вашего кода.\n",
    "        \n",
    "    def __call__(self, predicted, masks):\n",
    "        predicted, masks = crop_images(predicted, masks)\n",
    "        b, c, h, w = predicted.shape\n",
    "        if c != 1:\n",
    "            raise ValueError(\"{} не подходит для многоклассовой классификации\".format(type(self)))\n",
    "            \n",
    "        # predicted: float32, BCHW.\n",
    "        # masks: long, BHW.\n",
    "            \n",
    "        # Ваш код здесь.\n",
    "        \n",
    "        loss = ...\n",
    "        \n",
    "        # Конец вашего кода.\n",
    "        \n",
    "        return loss\n",
    "        \n",
    "seminar.check_bce_loss(BCELoss)\n",
    "seminar.check_ok()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class XentLoss(torch.nn.Module):\n",
    "    \"\"\"Функция потерь для сегментации с несколькими классами.\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self._xent = torch.nn.CrossEntropyLoss()\n",
    "        \n",
    "    def __call__(self, predicted, masks):\n",
    "        predicted, masks = crop_images(predicted, masks)\n",
    "        b, c, h, w = predicted.shape\n",
    "        if c == 1:\n",
    "            raise ValueError(\"{} не подходит для бинарной классификации\".format(type(self)))\n",
    "        predicted = predicted.permute(0, 2, 3, 1).reshape(b * h * w, c)\n",
    "        masks = masks.reshape(-1)\n",
    "        loss = self._xent(predicted, masks)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_optimizer(model):\n",
    "    return torch.optim.Adam(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = BCELoss if NUM_CLASSES == 2 else XentLoss\n",
    "\n",
    "if TRAIN:\n",
    "    trainset.set_padding(unet.padding)\n",
    "    valset.set_padding(unet.padding)\n",
    "    seminar.train_model(unet, loss_fn, make_optimizer, trainset, valset,\n",
    "                        lr_scheduler_fn=lambda opt: None,\n",
    "                        eval_steps=500,\n",
    "                        batch_size=4)\n",
    "else:\n",
    "    unet.load_state_dict(torch.load(\"checkpoint-noaug.pth.tar\", map_location=DEVICE))\n",
    "    cvmade.plot.show_image(\"resources/00-1-noaug.jpg\")\n",
    "    cvmade.plot.show_image(\"resources/00-2-noaug.jpg\")\n",
    "    cvmade.plot.show_image(\"resources/00-3-noaug.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(unet.state_dict(), \"checkpoint-noaug.pth.tar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir lightning_logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Show segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_segmentations(model, dataset, n=5):\n",
    "    model.eval()\n",
    "    if USE_CUDA:\n",
    "        model.cuda()\n",
    "    for _ in range(n):\n",
    "        image, mask_gt = dataset[random.randint(0, len(dataset) - 1)]\n",
    "        if USE_CUDA:\n",
    "            image = image.cuda()\n",
    "            mask_gt = mask_gt.cuda()\n",
    "        with torch.no_grad():\n",
    "            predicted = model(image.unsqueeze(0))[0]  # CHW.\n",
    "            predicted, image = crop_images(predicted.unsqueeze(0), image.unsqueeze(0))\n",
    "            predicted, image = predicted[0], image[0]\n",
    "            c = predicted.shape[0]\n",
    "            if c == 1:\n",
    "                predicted = torch.nn.functional.logsigmoid(predicted)\n",
    "                aggregated = predicted[0]\n",
    "                predicted_labels = predicted[0] > np.log(0.5)\n",
    "            else:\n",
    "                predicted = torch.nn.functional.log_softmax(predicted, 0)\n",
    "                aggregated = torch.logsumexp(predicted[1:], axis=0)\n",
    "                predicted_labels = predicted.max(0)[1]\n",
    "        aggregated = aggregated.cpu().numpy()\n",
    "        predicted_labels = predicted_labels.cpu().numpy().astype(np.uint8)\n",
    "        image = cvmade.image.image_to_numpy(image)\n",
    "        mask = (predicted_labels > 0)[..., None]\n",
    "        selected = image * mask + 255 * (1 - mask)\n",
    "\n",
    "        %matplotlib inline\n",
    "        if c != 1:\n",
    "            print(\"Classes:\", [LABELS[i] for i in set(predicted_labels.flatten().tolist())])\n",
    "        set_figure_size()\n",
    "        fig, axs = plt.subplots(1, 4, figsize=(12, 6))\n",
    "        axs[0].imshow(image)\n",
    "        axs[1].imshow(predicted_labels)\n",
    "        axs[2].imshow(aggregated)\n",
    "        axs[3].imshow(selected)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(\"Trainset\")\n",
    "show_segmentations(unet, trainset)\n",
    "\n",
    "print(\"Valset\")\n",
    "show_segmentations(unet, valset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Augmenation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset_augmented = VOCDataset(DATA_ROOT, VOC_YEAR, (IMAGE_SIZE, IMAGE_SIZE),\n",
    "                                multiclass=MULTICLASS, train=True, augment=True)\n",
    "show_dataset(trainset_augmented)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = BCELoss if NUM_CLASSES == 2 else XentLoss\n",
    "\n",
    "if TRAIN:\n",
    "    # Разрешить интерактивные графики.\n",
    "    trainset.set_padding(unet.padding)\n",
    "    valset.set_padding(unet.padding)\n",
    "    seminar.train_model(unet, loss_fn, make_optimizer, trainset_augmented, valset,\n",
    "                        lr_scheduler_fn=lambda optimizer: None,\n",
    "                        eval_steps=500,\n",
    "                        batch_size=4)\n",
    "    # Отключить интерактивные графики.\n",
    "else:\n",
    "    unet.load_state_dict(torch.load(\"checkpoint-aug.pth.tar\", map_location=DEVICE))\n",
    "    cvmade.plot.show_image(\"resources/01-1-aug.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(unet.state_dict(), \"checkpoint-aug.pth.tar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir lightning_logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(\"Trainset\")\n",
    "show_segmentations(unet, trainset)\n",
    "\n",
    "print(\"Valset\")\n",
    "show_segmentations(unet, valset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Предобученная сеть"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unet_pretrained = smp.Unet(\"resnet34\", encoder_weights=\"imagenet\", classes=NUM_CLASSES if NUM_CLASSES > 2 else 1)\n",
    "\n",
    "unet_pretrained.eval()\n",
    "sample_input = torch.zeros((1, 3, 1024, 1024))\n",
    "if USE_CUDA:\n",
    "    sample_input = sample_input.cuda()\n",
    "    unet_pretrained.cuda()\n",
    "with torch.no_grad():\n",
    "    sample_output = unet_pretrained(sample_input)\n",
    "pretrained_padding = (sample_input.shape[-1] - sample_output.shape[-1]) // 2\n",
    "print(unet_pretrained)\n",
    "print(\"Padding\", pretrained_padding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Сегментация перед тренировкой"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(\"Trainset\")\n",
    "show_segmentations(unet_pretrained, trainset)\n",
    "\n",
    "print(\"Valset\")\n",
    "show_segmentations(unet_pretrained, valset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Тренировка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = BCELoss if NUM_CLASSES == 2 else XentLoss\n",
    "\n",
    "if TRAIN:\n",
    "    trainset.set_padding(pretrained_padding)\n",
    "    valset.set_padding(pretrained_padding)\n",
    "    seminar.train_model(unet_pretrained, loss_fn, make_optimizer, trainset, valset,\n",
    "                        lr_scheduler_fn=lambda opt: None,\n",
    "                        eval_steps=500,\n",
    "                        batch_size=4)\n",
    "else:\n",
    "    unet_pretrained.load_state_dict(torch.load(\"checkpoint-pretrained.pth.tar\", map_location=DEVICE))\n",
    "    cvmade.plot.show_image(\"resources/02-pretrained.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(unet_pretrained.state_dict(), \"checkpoint-pretrained.pth.tar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir lightning_logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(\"Trainset\")\n",
    "show_segmentations(unet_pretrained, trainset)\n",
    "\n",
    "print(\"Valset\")\n",
    "show_segmentations(unet_pretrained, valset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
