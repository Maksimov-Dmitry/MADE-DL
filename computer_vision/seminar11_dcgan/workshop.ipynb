{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![MADE](resources/made.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Академия MADE\n",
    "\n",
    "\n",
    "# Семинар 11: GAN для генерации лиц\n",
    "\n",
    "Сегодня мы займемся задачей генерации лиц с помощью генеративно-состязательных сетей (GAN). Мы не будем применять никаких эвристик, связанных именно с лицами, поэтому полученный в итоге пайплайн можно будет использовать для генерации объектов иной природы, подставив нужный датасет (ну и, скорее всего, подправив какие-то из гиперпараметров).\n",
    "\n",
    "#### **План**:\n",
    "1. **GAN & картинки: Deep Convolutional GAN (DCGAN)**\n",
    "2. **DCGAN & BCELoss.**\n",
    "3. **Гиперпараметры для GAN.**\n",
    "4. **Анализ проблем и что делать дальше**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. GAN & картинки"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Tiny recap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Overview](resources/gan_overview.png)\n",
    "\n",
    "[отсюда](http://robocraft.ru/blog/machinelearning/3693.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В базовом случае ([Goodfellow et al, 2014](https://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf)) схема обучения GAN такова:\n",
    "- Есть две модели, генератор $G$ и дискриминатор $D$.\n",
    "- Генератор: \n",
    "  - На вход получает вектор шума $z$,\n",
    "  - На выходе показывает объект (например, картинку) $G(z)$.\n",
    "\n",
    "- Дискриминатор: \n",
    "  - На вход получает либо настоящий, либо сгенерированный объект ($x$, $G(z)$),\n",
    "  - На выходе отдает уверенность в том, что объект - настоящий ($D(x)$, $D(G(z))$).\n",
    "\n",
    "Обучение генератора и дискриминатора производится отдельными шагами:\n",
    "- \"Ошибка дискриминатора\" = **BCELoss**\n",
    "- На шаге обучения генератора ошибка дискриминатора **максимизируется** (градиентный подъем),\n",
    "- На шаге обучения дискриминатора ошибка дискриминатора **минимизируется** (good old градиентный спуск).\n",
    "\n",
    "На лекциях было показано, что при такой схеме обучения должно происходить \"сближение\" двух распределений - \"настоящего\" $p_{data}(x)$ и \"сгенерированного\" $p_{g}(x)$ - с дивергенцией Йенсена-Шэннона в качестве критерия близости."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Засовываем картинки в GAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Авторы оригинальной статьи использовали в экспериментах только полносвязные сети, в том числе и для генерации изображений (в низком разрешении).\n",
    "Как мы знаем, там, где нужна работа с картинками, ~~полносвязным сетям места нет~~ обычно более эффективными оказываются сверточные сети. Одной из первых публикаций по теме использования сверточных сетей для генерации изображений с помощью GAN была статья [\"Unsupervised Representation Learning With Deep Convolutional Generative Adversarial Networks\"](https://arxiv.org/pdf/1511.06434.pdf). Основные тезисы:\n",
    "* Не использовать слои \"грубого\" изменения размера карт активаций (`Pooling`, `Upsampling`); использовать сверточные слои (со `stride`> 1) и слои с транспонированными свертками,\n",
    "* Использовать `BatchNorm` в обеих моделях,\n",
    "* Не использовать полносвязные слои вообще,\n",
    "* В генераторе использовать `ReLU` и `Tanh` (в конце),\n",
    "* В дискриминаторе использовать `LeakyReLU`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![dcgan](resources/dcgan.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Кроме того, один из соавторов статьи про `DCGAN` и ключевая фигура в разработке `PyTorch` Soumith Chintala выложил [свои рекомендации](https://github.com/soumith/ganhacks) по обучению GAN. Некоторые из них мы используем в своих экспериментах, а именно:\n",
    "- Не использовать \"смешанные\" батчи (из настоящих и сгенерированных изображений), делать инференс по-отдельности,\n",
    "- Использовать \"soft labels\" (`0+eps` вместо `0`, `1.0-eps` вместо `1`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Далее соберем архитектуру, подобную `DCGAN`, обучим ее с небольшими изменениями и посмотрим, что из этого получится."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. (Baseline) DCGAN & BCELoss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "При установке `TRAIN` = `False` вместо обучения будут подгружаться веса модели и ожидаемые результаты."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Реализуем:\n",
    "1. Подгрузку данных\n",
    "2. Классы для дискриминатора и генератора\n",
    "3. Функцию для обучения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tqdm\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from IPython.display import HTML\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch import autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from seminar.utils import get_device, batch_to_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = get_device()\n",
    "print(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Данные"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Надо скачать [выровненные изображения лиц](https://cloud.mail.ru/public/ry5k/pQ8u9yut6) из датасета [CelebA](http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html). Затем распаковать и положить в папку `DATA_ROOT`.\n",
    "\n",
    "*Параметры и данные из п.2.1. будем использовать во всех дальнейших экспериментах без изменений.* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_ROOT = \"./data/img_align_celeba/\"\n",
    "\n",
    "IMAGE_SIZE = 64\n",
    "\n",
    "BATCH_SIZE = 512\n",
    "NUM_WORKERS = 8\n",
    "\n",
    "NUM_TO_SHOW = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from seminar.data import get_data  # goto\n",
    "from seminar.utils import show_data_batch, save_data_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset, dataloader = get_data(data_root=DATA_ROOT, batch_size=BATCH_SIZE, num_workers=NUM_WORKERS, image_size=IMAGE_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(dataloader))\n",
    "\n",
    "show_data_batch(batch, max_images=NUM_TO_SHOW)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Модели"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для состязательного обучения потребуются две модели - генератор и дискриминатор."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Зададим гиперпараметры моделей генератора и дискриминатора.\n",
    "* `LATENT_DIM`: размерность скрытого пространства (*latent space*), т.е. длина вектора шума, из которого мы будем получать лица с помощью генератора,\n",
    "* `IMAGE_CHANNELS`: число каналов в изображениях (мы будем использовать RGB-представление, поэтому каналов 3),\n",
    "* `*_BASE_FEATURES`: параметры, определяющие характерную ширину сверточных слоев в обеих моделях.\n",
    "* `*_NORMALIZATION`: флаги для выбора типа слоев нормализации данных внутри генератора и дискриминатора (`batch` или `none`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LATENT_DIM = 128\n",
    "IMAGE_CHANNELS = 3\n",
    "\n",
    "DISCRIMINATOR_BASE_FEATURES = 64\n",
    "GENERATOR_BASE_FEATURES = 64\n",
    "\n",
    "DISCRIMINATOR_NORMALIZATION = \"batch\"\n",
    "GENERATOR_NORMALIZATION = \"batch\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.1. Дискриминатор"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Начнем с дискриминатора. Учтя рекомендациям авторов `DCGAN`, его структуру сделаем следующей:\n",
    "* На вход подается изображение с заданным числом каналов (`input_channels`);\n",
    "* Тело состоит из последовательных блоков вида `Conv2d - BN2d  - LeakyReLU`; на выходе - линейное преобразование посредством свертки;\n",
    "* Сверточные слои сделаем со `stride=2`, т.к. мы не хотим использовать `MaxPool2d`; `kernel_size=4`, `padding=0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from seminar.models import Discriminator  # goto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Модели рассчитаны на работу с изображениями `64x64` (размер уменьшается в 64 раза); проверим, что на выходе получается одно-единственное число (уверенность дискриминатора в том, что пример - реальный):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator = Discriminator(input_channels=IMAGE_CHANNELS, base_features=DISCRIMINATOR_BASE_FEATURES, normalization=DISCRIMINATOR_NORMALIZATION)\n",
    "print(discriminator)\n",
    "\n",
    "x = torch.randn(4, IMAGE_CHANNELS, IMAGE_SIZE, IMAGE_SIZE)\n",
    "y = discriminator(x)\n",
    "assert y.size() == (x.size(0), 1), y.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.2. Генератор"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "С генератором чуть сложнее: на вход он получает вектор шума `z` длины `LATENT_DIM`, но в следующем виде: `LATENT_DIM x 1 x 1`.\n",
    "В отличие от дискриминатора, здесь используем транспонированные свертки, активации `ReLU` в середине и `Tanh` в самом конце."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Генератор тоже заточен под один размер изображений - `64x64`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from seminar.models import Generator  # goto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = Generator(input_channels=LATENT_DIM, base_features=GENERATOR_BASE_FEATURES, normalization=GENERATOR_NORMALIZATION, output_channels=IMAGE_CHANNELS)\n",
    "print(generator)\n",
    "\n",
    "x = torch.randn(4, LATENT_DIM, 1, 1)\n",
    "y = generator(x)\n",
    "assert y.size() == (x.size(0), IMAGE_CHANNELS, IMAGE_SIZE, IMAGE_SIZE), y.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Используем также функцию для ручной инициализации весов наших моделей:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init(m, scale=0.02):\n",
    "    if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d, nn.Linear)):\n",
    "        torch.nn.init.normal_(m.weight, 0.0, scale)\n",
    "    elif isinstance(m, nn.BatchNorm2d):\n",
    "        torch.nn.init.normal_(m.weight, 0.0, scale)\n",
    "        torch.nn.init.constant_(m.bias, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Код для обучения"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы генератор мог создавать изображения, на вход ему требуется подавать векторы шума. Напишем функцию, которая бы генерировала эти векторы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_noise(batch_size=BATCH_SIZE, latent_dim=LATENT_DIM, device=DEVICE):\n",
    "    \"\"\"Create batch_size normally distributed (0, I) vectors of length latent_dim.\n",
    "\n",
    "    Args:\n",
    "        - batch_size: Number of vectors to sample.\n",
    "        - latent_dim: Dimension of latent space (= length of noise vector).\n",
    "        - device: Device to store output on.\n",
    "\n",
    "    Returns:\n",
    "        Torch.Tensor of shape (batch_size, latent_dim, 1, 1).\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    noise = ...\n",
    "    # END OF YOUR CODE\n",
    "    return noise\n",
    "\n",
    "z = generate_noise(BATCH_SIZE, LATENT_DIM, DEVICE)\n",
    "\n",
    "assert z.size() == (BATCH_SIZE, LATENT_DIM, 1, 1), \"Dimensions don't match\"\n",
    "assert z.mean().abs() < 0.01, \"Mean of Z is not around zero, are you using normal distribution?\"\n",
    "assert (z.std() - 1).abs() < 0.01, \"Std of Z is not around 1, are you using normal distribution?\"\n",
    "assert z.device == DEVICE, \"Don't you forget to put output tensor to proper device.\"\n",
    "\n",
    "print(\"All checks passed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сгенерируем фиксированный вектор шума, чтобы отслеживать качество работы генератора на нем по мере обучения:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FIXED_NOISE = generate_noise(batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь к самому важному - функции потерь для обучения.\n",
    "\n",
    "Напомним алгоритм обучения GAN в [первозданном](https://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf) виде:\n",
    "\n",
    "![Algo](resources/gan_algo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для каждого батча из настоящих данных сначала обновляются веса дискриминатора (`k` раз, `k` >= 1), затем и веса генератора. \n",
    "\n",
    "Дискриминатор служит классификатором для отличения настоящих примеров (`real`, метка класса 1) от искусственных (`fake`, 0). В качестве функции потерь выступает бинарная кросс-энтропия (BCE), причем обновление весов дискриминатора происходит в направлении минимизации, а генератора - максимизации функции потерь."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Один из известных \"лайфхаков\" для обучения с BCE - это использовать вместо строгих 0 и 1 т.н. *smooth*-метки, как в ячейке ниже."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABEL_REAL = 0.95\n",
    "LABEL_FAKE = 0.05"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь реализуем функции для шага обучения дискриминатора и генератора с BCE-Loss по-отдельности. Сделаем это в виде одного класса с 2 необходимыми методами, интерфейс для которого описан в классе `GANLoss`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GANLoss(nn.Module):\n",
    "    \"\"\"Base class for GAN training loss functions.\"\"\"\n",
    "    \n",
    "    def generator(self, discriminator, generator, real_batch, fake_batch, device):\n",
    "        \"\"\"Make 1 training step for generator.\n",
    "        \n",
    "        Args:\n",
    "            - discriminator: Discriminator object.\n",
    "            - generator: Generator object.\n",
    "            - real_batch: Batch of real images (compatible with discriminator).\n",
    "            - real_batch: Batch of fake images from generator (compatible with discriminator).\n",
    "            - device: Device to use.\n",
    "            \n",
    "        Returns:\n",
    "            Torch.Tensor of size 1 with resulting loss value for generator.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def discriminator(self, discriminator, generator, real_batch, fake_batch, device):\n",
    "        \"\"\"Make 1 training step for generator.\n",
    "        \n",
    "        Args:\n",
    "            - discriminator: Discriminator object.\n",
    "            - generator: Generator object.\n",
    "            - real_batch: Batch of real images (compatible with discriminator).\n",
    "            - real_batch: Batch of fake images from generator (compatible with discriminator).\n",
    "            - device: Device to use.\n",
    "            \n",
    "        Returns:\n",
    "            Torch.Tensor of size 1 with resulting loss value for generator.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нам нужно реализовать по этому примеру класс для работы с BCE Loss.\n",
    "\n",
    "Для дискриминатора:\n",
    "- Пропустить через дискриминатор `real_batch` / `fake_batch`, получить активации.\n",
    "- Приготовить правильные метки для двух классов (используя `soft labels`, как описано выше).\n",
    "- Посчитать `BCELoss` (точнее, `BCEWithLogitsLoss`, поскольку нелинейности на выходе дискриминатора нет) для двух наборов активаций и вернуть среднее.\n",
    "\n",
    "Для генератора:\n",
    "- Пропустить через дискриминатор `fake_batch`, получить активации.\n",
    "- Приготовить \"неправильные\" метки для класса `fake` (то есть подсунуть метки класса `real`).\n",
    "- Посчитать `BCEWithLogitsLoss` и вернуть его значение."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.functional import binary_cross_entropy_with_logits\n",
    "\n",
    "class GANBCELoss(GANLoss):\n",
    "    \n",
    "    def generator(self, discriminator, generator, real_batch, fake_batch, device):\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "        prob_fake = ...\n",
    "        labels_fake = ...\n",
    "        generator_loss = ...\n",
    "        # END OF YOUR CODE\n",
    "        \n",
    "        return generator_loss\n",
    "\n",
    "    def discriminator(self, discriminator, generator, real_batch, fake_batch, device):\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "        prob_real = ...\n",
    "        prob_fake = ...\n",
    "        labels_real = ...\n",
    "        labels_fake = ...\n",
    "        discriminator_loss = ...\n",
    "        # END OF YOUR CODE\n",
    "    \n",
    "        return discriminator_loss\n",
    "\n",
    "    \n",
    "from seminar.losses import GANBCELoss as GANBCELossCorrect\n",
    "\n",
    "generator.to(DEVICE)\n",
    "discriminator.to(DEVICE)\n",
    "\n",
    "bce_loss = GANBCELoss()\n",
    "bce_loss_correct = GANBCELossCorrect()\n",
    "\n",
    "real_batch = next(iter(dataloader))\n",
    "fake_batch = generator(FIXED_NOISE.to(DEVICE))\n",
    "\n",
    "assert bce_loss.generator(discriminator, generator, real_batch, fake_batch, DEVICE) == \\\n",
    "    bce_loss_correct.generator(discriminator, generator, real_batch, fake_batch, DEVICE), \\\n",
    "    \"Wrong outputs for generator loss\"\n",
    "\n",
    "assert bce_loss.discriminator(discriminator, generator, real_batch, fake_batch, DEVICE) == \\\n",
    "    bce_loss_correct.discriminator(discriminator, generator, real_batch, fake_batch, DEVICE), \\\n",
    "    \"Wrong outputs for discriminator loss\"\n",
    "\n",
    "print(\"All checks passed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь, когда у нас есть составные компоненты, используем (более-менее) универсальную обвязку для обучения в рамках 1 эпохи."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from seminar.training import train_epoch  # goto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4. Обучение"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сети GAN могут быть очень чувствительны к изменениям гиперпараметров. О том, как можно упростить процесс их подбора, поговорим чуть позже, а пока используем рабочий набор гиперпараметров, с которым обучение должно быть стабильным и давать неплохой результат."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = 0.0003\n",
    "BETAS = [0.5, 0.999]\n",
    "N_CRITIC = 1\n",
    "\n",
    "NUM_EPOCHS = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = Generator().to(DEVICE)\n",
    "generator.apply(weights_init)\n",
    "optimizer_generator = optim.Adam(generator.parameters(), lr=LR, betas=BETAS, amsgrad=True)\n",
    "\n",
    "discriminator = Discriminator().to(DEVICE)\n",
    "discriminator.apply(weights_init)\n",
    "optimizer_discriminator = optim.Adam(discriminator.parameters(), lr=LR, betas=BETAS, amsgrad=True)\n",
    "\n",
    "loss_fn = GANBCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_name = \"00_bce_64x64_lr3e-4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_dirname = os.path.join(\"runs\", run_name)\n",
    "checkpoint_filename = os.path.join(run_dirname, \"checkpoint.pth.tar\")\n",
    "results_filename = os.path.join(run_dirname, \"results.pkl\")\n",
    "images_dirname = os.path.join(run_dirname, \"images\")\n",
    "\n",
    "if TRAIN:\n",
    "    \n",
    "    os.makedirs(run_dirname)\n",
    "    os.makedirs(images_dirname)\n",
    "    \n",
    "    results = {\n",
    "        key: [] for key in [\n",
    "            \"generator_loss_list\", \n",
    "            \"discriminator_loss_list\", \n",
    "            \"generated_batch_list\"\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        epoch_results = train_epoch(generator, discriminator, \n",
    "                                    optimizer_generator, optimizer_discriminator,\n",
    "                                    dataloader, epoch, NUM_EPOCHS, \n",
    "                                    LATENT_DIM, FIXED_NOISE, loss_fn, N_CRITIC, \n",
    "                                    DEVICE)\n",
    "        for key in results:\n",
    "            results[key].extend(epoch_results[key])\n",
    "            \n",
    "        with open(results_filename, \"wb\") as fp:\n",
    "            pickle.dump(results, fp)\n",
    "\n",
    "        with open(checkpoint_filename, \"wb\") as fp:\n",
    "            torch.save({\"generator\": generator.state_dict(),\n",
    "                        \"discriminator\": discriminator.state_dict()}, fp)\n",
    "        \n",
    "        image_filename = os.path.join(images_dirname, f\"batch_ep={str(epoch).zfill(2)}.png\")\n",
    "        save_data_batch(epoch_results[\"generated_batch_list\"][0], image_filename)\n",
    "\n",
    "else:\n",
    "    print(\"%3 hours later%\")\n",
    "    with open(results_filename, \"rb\") as fp:\n",
    "        results = pickle.load(fp)\n",
    "    with open(checkpoint_filename, \"rb\") as fp:\n",
    "        checkpoint = torch.load(fp, map_location=\"cpu\")\n",
    "    generator.load_state_dict(checkpoint[\"generator\"])\n",
    "    discriminator.load_state_dict(checkpoint[\"discriminator\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим на графики значений лосса для обеих моделей:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,5))\n",
    "plt.title(\"Generator and Discriminator Loss During Training\")\n",
    "plt.plot(results[\"generator_loss_list\"], label=\"G\", alpha=0.5)\n",
    "plt.plot(results[\"discriminator_loss_list\"], label=\"D\", alpha=0.5)\n",
    "plt.xlabel(\"iterations\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Минус обучения GAN с BCE в том, что графики лосса плохо отражают суть происходящего с моделями. С одной стороны, если значения лоссов сильно отличается друг от друга (на порядок), то это может говорить об остановке обучения и т.н. коллапсе. Однако, даже если \"на глаз\" графики кажутся \"нормальными\", это не говорит о том, что генератор в самом деле выдает что-то хорошее.\n",
    "\n",
    "Поэтому лучше было бы иметь некую дополнительную (или несколько) метрику, которая бы отражала прогресс обучения генератора. В ее качестве можно взять `Frechet Inception Distance`, о ней поговорим чуть позже."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Отрисуем сгенерированные примеры, полученные на разных эпохах обучения из вектора `FIXED_NOISE`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%capture\n",
    "fig = plt.figure(figsize=(16,16))\n",
    "plt.axis(\"off\")\n",
    "imgs = [[plt.imshow(batch_to_image(batch[:64]), animated=True)] for batch in results[\"generated_batch_list\"][::8]]\n",
    "ani = animation.ArtistAnimation(fig, imgs, interval=1000, repeat_delay=1000, blit=True)\n",
    "\n",
    "HTML(ani.to_jshtml())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Генератор, как видно, двигался в правильном направлении - он действительно стал выдавать что-то похожее на лица. Однако количество артефактов довольно велико, и не все из результатов можно назвать \"адекватными\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "С помощью ячейки ниже можно погенерировать примеров и поисследовать характерные артефакты."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise = generate_noise(batch_size=64)\n",
    "\n",
    "generator.eval()\n",
    "with torch.no_grad():\n",
    "    generated = generator(noise)\n",
    "\n",
    "show_data_batch(generated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Частые проблемы генератора:\n",
    "* Нет консистентности частей лица (например, мужские лица + женские прически)\n",
    "* Нереалистичная форма лица\n",
    "* Сильный шум на фоне\n",
    "* Искаженные лица"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Немного поиграем с получившимся генератором. Возьмем два батча векторов шума и выполним линейную интерполяцию между ними:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpolate_noise(batch1, batch2, num):\n",
    "    \"\"\"Interpolate between two tensors of the same shape.\n",
    "\n",
    "    Args:\n",
    "        - noise1: First ('start') tensor (shaped b x latent_dim x *).\n",
    "        - noise2: Second ('end') tensor (shaped b x latent_dim x *).\n",
    "        - num: Number of points in interpolated output.\n",
    "\n",
    "    Returns:\n",
    "        List of num torch.Tensors (each shaped b x latent_dim x *).\n",
    "    \"\"\"\n",
    "    \n",
    "    step_size = 1 / (num - 1)\n",
    "    weights = torch.tensor([i * step_size for i in range(num)])\n",
    "    interpolation_list = [torch.lerp(noise1, noise2, w) for w in weights]\n",
    "    \n",
    "    return interpolation_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator.eval()\n",
    "\n",
    "noise1 = generate_noise(batch_size=8, device=\"cpu\")\n",
    "noise2 = generate_noise(batch_size=8, device=\"cpu\")\n",
    "interpolated = interpolate_noise(noise1, noise2, num=8)\n",
    "\n",
    "for i in range(len(interpolated)):\n",
    "    noise = interpolated[i]\n",
    "    with torch.no_grad():\n",
    "        generated = generator(noise.to(DEVICE))\n",
    "    show_data_batch(generated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Что дальше?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Есть несколько вариантов того, что можно сделать для улучшения результатов:\n",
    "1. Более тщательно подобрать гиперпараметры в текущем подходе.\n",
    "2. Использовать альтернативные функции потерь, о которых можно узнать из лекций / статей / etc (например, `Wasserstein Loss` или `Hinge Loss`).\n",
    "3. Внести изменения в архитектуру моделей, направленные на улучшение качества генерации или устойчивости обучения (`Self-Attention`, `Spectral Normalization`, ...).\n",
    "\n",
    "В любом случае, процесс улучшения GAN будет трудоемким: изменение функции потерь (п.2) или архитектуры (п.3) может потребовать заново подбирать гиперпараметры (п.1) уже настроенного пайплайна.\n",
    "Поэтому уделим время тому, как это можно сделать с меньшими трудозатратами."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Optuna"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На одном из прошлых занятий обсуждался фреймворк для оптимизации гиперпараметров [Optuna](https://optuna.org/). Коротко напомним, как с ним работать:\n",
    "\n",
    "1. Заменить явное присвоение значений интересующих нас гиперпараметров на вызовы вида `trial.suggest_int` (`_float`, `_categorical`, ...), указав области значений (нижнюю/верхнюю границы или список категорий). Optuna будет в каждом испытании (`Trial`) подставлять значение из данной области.\n",
    "2. Обернуть обучение в целевую функцию и передать ее для оптимизации специальному объекту `optuna.Study`, который сам запустит указанное число испытаний и соберет итоговые результаты.\n",
    "\n",
    "Целевая функция должна возвращать значение, которое optuna будет использовать для оценки данного набора гиперпараметров. Оно должно отражать качество работы ваших моделей; в случае с GAN, это должна быть метрика качества генерации. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Frechet Inception Distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Распространенным подходом к оценке качества генерации является `Frechet Inception Distance` (`FID`) между наборами настоящих и сгенерированных изображений.\n",
    " \n",
    "* `Frechet Distance` - это показатель для сравнения двух распределений $p_1$($\\mu_1$, $\\Sigma_1$) и $p_2$($\\mu_2$, $\\Sigma_2$); вычисляется как $$fd^2 = ||\\mu_1 - \\mu_2||^2 + Tr(\\Sigma_1 + \\Sigma_2 - 2\\sqrt{\\Sigma_1 \\Sigma_2})$$\n",
    "* Чем ближе `FD` к нулю, тем более \"похожи\" распределения.\n",
    "* В качестве случайных величин в FID используются активации слоев для feature_extraction предобученного (на ImageNet) классификатора Inception; отсюда `I` в `FID`.\n",
    "* [Как показывает практика](https://arxiv.org/pdf/1801.03924.pdf), признаки из предобученных на ImageNet моделях неплохо коррелируют с человеческим восприятием \"похожести\", что позволяет надеяться на FID как на адекватную метрику качества генерации реалистичных изображений. \n",
    "\n",
    "То есть алгоритм оценки `FID` на наборе реальных данных такой:\n",
    "1. Сгененировать с помощью модели генератора выборку \"фейковых\" данных.\n",
    "2. Пропустить ее через предобученную модель Inception и сохранить активации необходимых слоев для feature_extraction.\n",
    "3. Пропустить также выборку реальных данных.\n",
    "4. Посчитать `FD` между распределениями активаций для двух доменов.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Реализацию вычисления `FID` (и других метрик) можно найти, например, в [репозитории](https://github.com/toshas/torch-fidelity) `torch_fidelity`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train-opt.py  # goto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. GAN Readlist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Статьи:\n",
    "1. [Generative Adversarial Networks](https://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf) (GAN)\n",
    "2. [Conditional GAN](https://arxiv.org/abs/1411.1784) (cGAN)\n",
    "3. [Unsupervised representation learning with deep convolutional generative adversarial networks](https://arxiv.org/pdf/1511.06434) (DCGAN)\n",
    "4. [Image-to-Image Translation with Conditional Adversarial Networks](https://arxiv.org/abs/1611.07004) [Pix2Pix]\n",
    "5. [Wasserstein GAN](https://arxiv.org/abs/1701.07875) (Wasserstein GAN)\n",
    "6. [Improved Training of Wasserstein GANs](https://arxiv.org/abs/1704.00028) (Wasserstein GAN + Gradient Penalty)\n",
    "7. [Spectral Normalization for Generative Adversarial Networks](https://arxiv.org/abs/1802.05957) (SNGAN)\n",
    "8. [Self-Attention Generative Adverarial Networks](https://arxiv.org/pdf/1805.08318) (SAGAN)\n",
    "9. [Large Scale GAN Training for High Fidelity Natural Image Synthesis](https://arxiv.org/abs/1809.11096) (BigGAN)\n",
    "10. [A Style-Based Generator Architecture for Generative Adversarial Networks](https://arxiv.org/abs/1812.04948) (StyleGAN)\n",
    "\n",
    "Репозитории и код:\n",
    "1. [PyTorch DCGAN example](https://pytorch.org/tutorials/beginner/dcgan_faces_tutorial.html) (основа для этого семинара)\n",
    "2. [PyTorch-GAN](https://github.com/eriklindernoren/PyTorch-GAN) (простые реализации множества статей по GAN)\n",
    "3. [BigGAN](https://github.com/eriklindernoren/PyTorch-GAN) (реализация огромного числа фич для GAN)\n",
    "4. [BigGAN TF Hub Demo](https://colab.research.google.com/github/tensorflow/hub/blob/master/examples/colab/biggan_generation_with_tf_hub.ipynb) (ноутбук с предобученным BigGAN)\n",
    "5. [How to Train a GAN? Tips and tricks to make GANs work](https://github.com/soumith/ganhacks) (осторожно, многие советы устарели)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
